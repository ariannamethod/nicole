#!/usr/bin/env python3
"""
Nicole2Nicole - –ú–æ–¥—É–ª—å –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –≤–µ—Å–æ–≤
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–º–µ—Ä—Ç–∏/–ø–µ—Ä–µ—Ä–æ–∂–¥–µ–Ω–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤.
–£—á–∏—Ç—Å—è –Ω–∞ –ª–æ–≥–∞—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –∏ —ç–≤–æ–ª—é—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.
"""

import sqlite3
import json
import time
import threading
import math
import random
import sys
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import h2o
# import nicole  # –£–±–∏—Ä–∞–µ–º —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–π –∏–º–ø–æ—Ä—Ç

@dataclass 
class LearningPattern:
    """–ü–∞—Ç—Ç–µ—Ä–Ω –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏"""
    input_pattern: str
    output_pattern: str
    metrics_context: Dict
    architecture_context: Dict
    success_score: float
    frequency: int = 1
    
class Nicole2NicoleCore:
    """–Ø–¥—Ä–æ —Å–∏—Å—Ç–µ–º—ã –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, memory_db: str = "nicole_memory.db", knowledge_file: str = "nicole_learned_knowledge.json"):
        self.memory_db = memory_db
        self.knowledge_file = knowledge_file  # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Auto-save —Ñ–∞–π–ª
        self.learning_patterns = {}
        self.evolution_patterns = {}
        self.architecture_preferences = {}
        self.learning_lock = threading.Lock()
        self.is_learning = False
        self.learning_cycles = 0  # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°—á–µ—Ç—á–∏–∫ —Ü–∏–∫–ª–æ–≤ –¥–ª—è auto-save

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Auto-load –∑–Ω–∞–Ω–∏–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        if os.path.exists(self.knowledge_file):
            print(f"[Nicole2Nicole] Loading previous knowledge from {self.knowledge_file}...")
            self.import_learned_knowledge(self.knowledge_file)
        
    def analyze_conversation_logs(self, limit: int = 1000) -> List[LearningPattern]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        conn = sqlite3.connect(self.memory_db)
        cursor = conn.cursor()
        
        cursor.execute("""
        SELECT user_input, nicole_output, metrics, transformer_config, timestamp
        FROM conversations 
        ORDER BY timestamp DESC 
        LIMIT ?
        """, (limit,))
        
        rows = cursor.fetchall()
        conn.close()
        
        patterns = []
        for row in rows:
            user_input, nicole_output, metrics_str, config_str, timestamp = row
            
            try:
                metrics = json.loads(metrics_str)
                config = json.loads(config_str)
                
                # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –æ–±—É—á–µ–Ω–∏—è
                pattern = LearningPattern(
                    input_pattern=self._extract_input_pattern(user_input),
                    output_pattern=self._extract_output_pattern(nicole_output),
                    metrics_context=metrics,
                    architecture_context=config,
                    success_score=self._calculate_success_score(metrics)
                )
                
                patterns.append(pattern)
                
            except Exception as e:
                print(f"[Nicole2Nicole] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–∞: {e}")
                
        return patterns
        
    def _extract_input_pattern(self, user_input: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞"""
        words = user_input.lower().split()
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π
        if any(word in words for word in ['–ø—Ä–∏–≤–µ—Ç', 'hello', 'hi', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π']):
            return "greeting"
        elif any(word in words for word in ['–ø–æ–∫–∞', 'bye', 'goodbye', '—É–≤–∏–¥–∏–º—Å—è']):
            return "farewell"  
        elif any(word in words for word in ['–∫–∞–∫', '–¥–µ–ª–∞', '–∂–∏–∑–Ω—å', '–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ']):
            return "status_inquiry"
        elif any(word in words for word in ['—á—Ç–æ', '—Ä–∞—Å—Å–∫–∞–∂–∏', '–æ–±—ä—è—Å–Ω–∏', '–æ–ø–∏—à–∏']):
            return "information_request"
        elif any(word in words for word in ['–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–æ—Ç—á–µ–≥–æ']):
            return "reasoning_request"
        elif len(words) > 10:
            return "long_message"
        elif len(words) < 3:
            return "short_message"
        else:
            return "general_conversation"
            
    def _extract_output_pattern(self, nicole_output: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –æ—Ç–≤–µ—Ç–∞ Nicole"""
        words = nicole_output.lower().split()
        
        if any(word in words for word in ['–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '—Ä–∞—Å—Å–∫–∞–∂–∏', '–±–æ–ª—å—à–µ']):
            return "curiosity_response"
        elif any(word in words for word in ['–ø–æ–Ω–∏–º–∞—é', '—Å–æ–≥–ª–∞—Å–Ω–∞', '–¥–∞']):
            return "agreement_response"
        elif any(word in words for word in ['–¥—É–º–∞—Ç—å', '—Å–ª–æ–∂–Ω—ã–π', '–≤–æ–ø—Ä–æ—Å']):
            return "contemplative_response"
        elif any(word in words for word in ['—á—Ç–æ', '–¥—É–º–∞–µ—à—å', '–º–Ω–µ–Ω–∏–µ']):
            return "question_back"
        else:
            return "general_response"
            
    def _calculate_success_score(self, metrics: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"""
        try:
            entropy = metrics.get('entropy', 0.5)
            perplexity = metrics.get('perplexity', 1.0)
            resonance = metrics.get('resonance', 0.3)
            coherence = metrics.get('coherence', 0.5)
            engagement = metrics.get('engagement', 0.5)
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
            score = (
                entropy * 0.2 +
                (1.0 / max(0.1, perplexity)) * 0.3 +
                resonance * 0.3 +
                coherence * 0.1 +
                engagement * 0.1
            )
            
            return min(1.0, max(0.0, score))
            
        except Exception:
            return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            
    def learn_from_patterns(self, patterns: List[LearningPattern]):
        """–û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"""
        with self.learning_lock:
            self.is_learning = True
            
            try:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ —Ç–∏–ø–∞–º
                pattern_groups = {}
                for pattern in patterns:
                    key = (pattern.input_pattern, pattern.output_pattern)
                    if key not in pattern_groups:
                        pattern_groups[key] = []
                    pattern_groups[key].append(pattern)
                    
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É
                for (input_type, output_type), group in pattern_groups.items():
                    self._analyze_pattern_group(input_type, output_type, group)
                    
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–≤–æ–ª—é—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
                self._analyze_architecture_evolution(patterns)
                
                print(f"[Nicole2Nicole] –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –Ω–∞ {len(patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö")
                
            finally:
                self.is_learning = False
                
    def _analyze_pattern_group(self, input_type: str, output_type: str, patterns: List[LearningPattern]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä—É–ø–ø—É –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if len(patterns) < 2:
            return
            
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        avg_metrics = {}
        metric_keys = ['entropy', 'perplexity', 'resonance', 'coherence', 'engagement']
        
        for key in metric_keys:
            values = []
            for pattern in patterns:
                if key in pattern.metrics_context:
                    values.append(pattern.metrics_context[key])
            if values:
                avg_metrics[key] = sum(values) / len(values)
                
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞
        best_patterns = sorted(patterns, key=lambda p: p.success_score, reverse=True)[:3]
        best_architectures = [p.architecture_context for p in best_patterns]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
        pattern_key = f"{input_type}::{output_type}"
        self.learning_patterns[pattern_key] = {
            'avg_metrics': avg_metrics,
            'best_architectures': best_architectures,
            'frequency': len(patterns),
            'avg_success': sum(p.success_score for p in patterns) / len(patterns)
        }
        
        print(f"[Nicole2Nicole] –ü–∞—Ç—Ç–µ—Ä–Ω {pattern_key}: {len(patterns)} –ø—Ä–∏–º–µ—Ä–æ–≤, —É—Å–ø–µ—à–Ω–æ—Å—Ç—å {self.learning_patterns[pattern_key]['avg_success']:.3f}")
        
    def _analyze_architecture_evolution(self, patterns: List[LearningPattern]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä"""
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        arch_performance = {}
        
        for pattern in patterns:
            arch = pattern.architecture_context
            for param, value in arch.items():
                if param not in arch_performance:
                    arch_performance[param] = []
                arch_performance[param].append((value, pattern.success_score))
                
        # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        for param, value_scores in arch_performance.items():
            if len(value_scores) < 5:
                continue
                
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
            sorted_values = sorted(value_scores, key=lambda x: x[1], reverse=True)
            top_20_percent = sorted_values[:max(1, len(sorted_values) // 5)]
            
            # –ù–∞—Ö–æ–¥–∏–º –¥–∏–∞–ø–∞–∑–æ–Ω –ª—É—á—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            top_values = [v[0] for v in top_20_percent]
            
            if isinstance(top_values[0], (int, float)):
                min_val = min(top_values)
                max_val = max(top_values)
                avg_val = sum(top_values) / len(top_values)
                
                self.architecture_preferences[param] = {
                    'min': min_val,
                    'max': max_val,
                    'avg': avg_val,
                    'samples': len(top_values)
                }
                
        print(f"[Nicole2Nicole] –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è {len(self.architecture_preferences)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
    def suggest_architecture_improvements(self, current_arch: Dict, conversation_context: str) -> Dict:
        """
        –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è + exploration noise

        ANTI-OVERFITTING: –¥–æ–±–∞–≤–ª—è–µ—Ç 10% —à–∞–Ω—Å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        """
        if not self.learning_patterns or not self.architecture_preferences:
            return current_arch

        improved_arch = current_arch.copy()

        # EXPLORATION NOISE: 10% —à–∞–Ω—Å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏–µ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –æ–ø—Ç–∏–º—É–º–µ
        exploration_probability = 0.1

        if random.random() < exploration_probability:
            # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            explorable_params = [p for p in improved_arch.keys()
                               if isinstance(improved_arch[p], (int, float))]

            if explorable_params:
                param_to_explore = random.choice(explorable_params)
                current_value = improved_arch[param_to_explore]

                # –°–ª—É—á–∞–π–Ω–æ–µ –≤–æ–∑–º—É—â–µ–Ω–∏–µ ¬±20%
                noise_factor = random.uniform(0.8, 1.2)
                improved_arch[param_to_explore] = current_value * noise_factor

                print(f"[Nicole2Nicole:Exploration] üé≤ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ '{param_to_explore}': "
                      f"{current_value:.3f} ‚Üí {improved_arch[param_to_explore]:.3f}")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑—É—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
        for param, preferences in self.architecture_preferences.items():
            if param in improved_arch:
                current_val = improved_arch[param]
                
                # –î–≤–∏–≥–∞–µ–º—Å—è –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É
                if isinstance(current_val, (int, float)):
                    target_val = preferences['avg']
                    
                    # –ü–ª–∞–≤–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ –∫ —Ü–µ–ª–∏
                    if current_val < preferences['min']:
                        improved_arch[param] = min(preferences['max'], current_val * 1.1)
                    elif current_val > preferences['max']:
                        improved_arch[param] = max(preferences['min'], current_val * 0.9)
                    else:
                        # –ú–µ–ª–∫–∏–µ —Å–ª—É—á–∞–π–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
                        noise = random.uniform(-0.05, 0.05)
                        improved_arch[param] = current_val * (1 + noise)
                        
        return improved_arch
        
    def suggest_response_strategy(self, user_input: str, current_metrics: Dict) -> str:
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑—É—á–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        input_pattern = self._extract_input_pattern(user_input)
        
        # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        matching_patterns = []
        for pattern_key, pattern_data in self.learning_patterns.items():
            if pattern_key.startswith(input_pattern + "::"):
                matching_patterns.append((pattern_key, pattern_data))
                
        if not matching_patterns:
            return "general_response"
            
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
        best_pattern = max(matching_patterns, key=lambda x: x[1]['avg_success'])
        output_pattern = best_pattern[0].split("::")[-1]
        
        return output_pattern
        
    def continuous_learning_loop(self):
        """
        –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
        –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Auto-save –∫–∞–∂–¥—ã–µ 10 —Ü–∏–∫–ª–æ–≤ (~5 –º–∏–Ω—É—Ç)
        """
        while True:
            try:
                if not self.is_learning:
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –ª–æ–≥–∏ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
                    patterns = self.analyze_conversation_logs(100)
                    if patterns:
                        self.learn_from_patterns(patterns)
                        self.learning_cycles += 1

                        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Auto-save –∫–∞–∂–¥—ã–µ 10 —Ü–∏–∫–ª–æ–≤
                        if self.learning_cycles % 10 == 0:
                            print(f"[Nicole2Nicole] Auto-saving knowledge (cycle {self.learning_cycles})...")
                            self.export_learned_knowledge(self.knowledge_file)

                time.sleep(30)

            except Exception as e:
                print(f"[Nicole2Nicole:ERROR] –û—à–∏–±–∫–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
                time.sleep(60)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
                
    def start_continuous_learning(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        learning_thread = threading.Thread(target=self.continuous_learning_loop, daemon=True)
        learning_thread.start()
        print("[Nicole2Nicole] –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ")
        
    def get_learning_statistics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è"""
        return {
            'learned_patterns': len(self.learning_patterns),
            'architecture_preferences': len(self.architecture_preferences),
            'is_learning': self.is_learning,
            'top_patterns': sorted(
                [(k, v['avg_success'], v['frequency']) for k, v in self.learning_patterns.items()],
                key=lambda x: x[1] * x[2],
                reverse=True
            )[:10]
        }
        
    def force_learning_session(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Å—Å–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        print("[Nicole2Nicole] –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        patterns = self.analyze_conversation_logs(500)
        if patterns:
            self.learn_from_patterns(patterns)
            return True
        return False
        
    def export_learned_knowledge(self, filepath: str):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –≤ —Ñ–∞–π–ª"""
        knowledge = {
            'learning_patterns': self.learning_patterns,
            'evolution_patterns': self.evolution_patterns,
            'architecture_preferences': self.architecture_preferences,
            'export_timestamp': time.time()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(knowledge, f, ensure_ascii=False, indent=2)
            
        print(f"[Nicole2Nicole] –ó–Ω–∞–Ω–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filepath}")
        
    def import_learned_knowledge(self, filepath: str):
        """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                knowledge = json.load(f)
                
            self.learning_patterns.update(knowledge.get('learning_patterns', {}))
            self.evolution_patterns.update(knowledge.get('evolution_patterns', {}))
            self.architecture_preferences.update(knowledge.get('architecture_preferences', {}))
            
            print(f"[Nicole2Nicole] –ó–Ω–∞–Ω–∏—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏–∑ {filepath}")
            return True
            
        except Exception as e:
            print(f"[Nicole2Nicole:ERROR] –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
            return False

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ—Å–Ω–æ–≤–Ω–æ–π Nicole —Å–∏—Å—Ç–µ–º–æ–π
class EnhancedFluidTransformer:  # –£–±–∏—Ä–∞–µ–º –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç nicole.FluidTransformer
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º"""
    
    def __init__(self, transformer_id: str, session_context: Dict = None, learning_core = None):
        # –£–±–∏—Ä–∞–µ–º super() –≤—ã–∑–æ–≤
        self.transformer_id = transformer_id
        self.session_context = session_context or {}
        self.learning_core = learning_core
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑—É—á–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        if self.learning_core:
            improved_arch = self.learning_core.suggest_architecture_improvements(
                self.architecture, 
                session_context.get('conversation_context', '')
            )
            self.architecture = improved_arch
            
    def evolve_architecture(self, metrics):
        """–≠–≤–æ–ª—é—Ü–∏—è —Å —É—á–µ—Ç–æ–º –∏–∑—É—á–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        # –°–Ω–∞—á–∞–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —ç–≤–æ–ª—é—Ü–∏—è
        evolved = super().evolve_architecture(metrics)
        
        # –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º –∏–∑—É—á–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
        if self.learning_core and evolved:
            learned_improvements = self.learning_core.suggest_architecture_improvements(
                self.architecture,
                ""  # –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            )
            
            # –ú—è–≥–∫–æ –ø—Ä–∏–º–µ–Ω—è–µ–º –∏–∑—É—á–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
            for param, target_val in learned_improvements.items():
                if param in self.architecture and isinstance(target_val, (int, float)):
                    current_val = self.architecture[param]
                    # –î–≤–∏–∂–µ–º—Å—è –Ω–∞ 10% –∫ –∏–∑—É—á–µ–Ω–Ω–æ–º—É –æ–ø—Ç–∏–º—É–º—É
                    self.architecture[param] = current_val * 0.9 + target_val * 0.1
                    
        return evolved

class EnhancedNicoleCore:  # –£–±–∏—Ä–∞–µ–º –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —è–¥—Ä–æ Nicole —Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º"""
    
    def __init__(self):
        # –£–±–∏—Ä–∞–µ–º super() - —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
        self.learning_core = Nicole2NicoleCore()
        self.learning_core.start_continuous_learning()
        
    def _spawn_new_transformer(self):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º"""
        transformer_id = f"learned_{self.session_id}_{int(time.time() * 1000000)}"
        
        # –£–±–∏–≤–∞–µ–º —Å—Ç–∞—Ä—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
        if self.current_transformer:
            self._kill_current_transformer()
            
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º
        self.current_transformer = EnhancedFluidTransformer(
            transformer_id, 
            {'session_id': self.session_id},
            self.learning_core
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–∏–ø—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        transformer_script = self.current_transformer.generate_transformer_script()
        
        try:
            self.h2o_engine.run_transformer_script(
                transformer_script, 
                transformer_id,
                {'session_context': self.current_transformer.session_context}
            )
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ
            self.memory.log_transformer_lifecycle(
                transformer_id,
                self.session_id,
                self.current_transformer.architecture,
                self.current_transformer.creation_time
            )
            
            print(f"[Nicole] –ù–æ–≤—ã–π –æ–±—É—á–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä {transformer_id} —Å–æ–∑–¥–∞–Ω")
            
        except Exception as e:
            print(f"[Nicole:ERROR] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞: {e}")
            
    def _generate_simple_response(self, user_input: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —É—á–µ—Ç–æ–º –∏–∑—É—á–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è
        suggested_strategy = self.learning_core.suggest_response_strategy(
            user_input, 
            self.current_transformer.current_metrics.__dict__ if self.current_transformer else {}
        )
        
        # –í—ã–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏–∑—É—á–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        response_strategies = {
            'curiosity_response': [
                "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —Ä–∞—Å—Å–∫–∞–∂–∏ –±–æ–ª—å—à–µ",
                "–≠—Ç–æ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ, –ø—Ä–æ–¥–æ–ª–∂–∞–π",
                "–•–æ—á—É —É–∑–Ω–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏"
            ],
            'agreement_response': [
                "–ü–æ–Ω–∏–º–∞—é —Ç–µ–±—è",
                "–°–æ–≥–ª–∞—Å–Ω–∞ —Å —Ç–æ–±–æ–π", 
                "–î–∞, —Ç—ã –ø—Ä–∞–≤"
            ],
            'contemplative_response': [
                "–•–º, –Ω—É–∂–Ω–æ –ø–æ–¥—É–º–∞—Ç—å",
                "–≠—Ç–æ —Å–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å",
                "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –º—ã—Å–ª—å"
            ],
            'question_back': [
                "–ê —á—Ç–æ —Ç—ã –æ–± —ç—Ç–æ–º –¥—É–º–∞–µ—à—å?",
                "–ö–∞–∫ —Ç—ã –∫ —ç—Ç–æ–º—É –æ—Ç–Ω–æ—Å–∏—à—å—Å—è?",
                "–ö–∞–∫–æ–µ —É —Ç–µ–±—è –º–Ω–µ–Ω–∏–µ?"
            ],
            'general_response': [
                "–ü–æ–Ω—è—Ç–Ω–æ",
                "–•–æ—Ä–æ—à–æ",
                "–Ø—Å–Ω–æ"
            ]
        }
        
        responses = response_strategies.get(suggested_strategy, response_strategies['general_response'])
        
        # –í—ã–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç —Å —ç–ª–µ–º–µ–Ω—Ç–æ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
        input_hash = hash(user_input.lower())
        response_idx = input_hash % len(responses)
        
        return responses[response_idx]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä Enhanced Nicole
enhanced_nicole = EnhancedNicoleCore()

def chat_with_enhanced_nicole(message: str) -> str:
    """–û–±—â–µ–Ω–∏–µ —Å –æ–±—É—á–µ–Ω–Ω–æ–π Nicole"""
    if not enhanced_nicole.session_id:
        enhanced_nicole.start_conversation()
        
    return enhanced_nicole.process_message(message)

def test_nicole2nicole():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
    print("=== NICOLE2NICOLE LEARNING TEST ===")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    test_session = enhanced_nicole.start_conversation("learning_test")
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
    test_conversations = [
        ("–ü—Ä–∏–≤–µ—Ç!", "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"),
        ("–ö–∞–∫ –¥–µ–ª–∞?", "–•–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ!"),
        ("–ß—Ç–æ –Ω–æ–≤–æ–≥–æ?", "–ú–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç"),
        ("–†–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ", "–Ø - Nicole, –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å"),
        ("–ü–æ–∫–∞!", "–î–æ –≤—Å—Ç—Ä–µ—á–∏!")
    ]
    
    for user_msg, expected_response in test_conversations:
        response = enhanced_nicole.process_message(user_msg)
        print(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_msg}")
        print(f"Nicole: {response}")
        time.sleep(0.1)
        
    enhanced_nicole.end_conversation()
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    print("\\n--- –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è ---")
    enhanced_nicole.learning_core.force_learning_session()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = enhanced_nicole.learning_core.get_learning_statistics()
    print(f"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è:")
    print(f"- –ò–∑—É—á–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {stats['learned_patterns']}")
    print(f"- –ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: {stats['architecture_preferences']}")
    print(f"- –¢–æ–ø –ø–∞—Ç—Ç–µ—Ä–Ω—ã:")
    for pattern, success, freq in stats['top_patterns']:
        print(f"  {pattern}: —É—Å–ø–µ—à–Ω–æ—Å—Ç—å={success:.3f}, —á–∞—Å—Ç–æ—Ç–∞={freq}")
        
    print("\\n=== NICOLE2NICOLE TEST COMPLETED ===")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_nicole2nicole()
    else:
        print("Nicole2Nicole Learning Engine –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        print("–î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python3 nicole2nicole.py test")
