#!/usr/bin/env python3
"""
HIGH.PY - Mathematical Brain of Nicole System
–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä Julia –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π

Nicole –∏—Å–ø–æ–ª—å–∑—É–µ—Ç high.py –¥–ª—è:
- –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –º–µ—Ç—Ä–∏–∫ (—ç–Ω—Ç—Ä–æ–ø–∏—è, —Ä–µ–∑–æ–Ω–∞–Ω—Å, –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è)
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
- –î–æ–æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –≤–µ—Å–æ–≤ —á–µ—Ä–µ–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
- –ë—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ n-–≥—Ä–∞–º–º–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∏—Å—Ç–∞–Ω—Ü–∏–π
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏

–§–∏–ª–æ—Å–æ—Ñ–∏—è: Julia - –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–∑–≥ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ 100x
"""

import os
import sys
import subprocess
import tempfile
import threading
import time
import math
import random
# import numpy as np  # –£–ë–†–ê–ù–û: –∑–∞–º–µ–Ω–µ–Ω–æ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
import json
import re

# –î–æ–±–∞–≤–ª—è–µ–º nicole2julia –≤ –ø—É—Ç—å –¥–ª—è Julia –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
NICOLE2JULIA_PATH = Path(__file__).parent / "nicole2julia"
sys.path.insert(0, str(NICOLE2JULIA_PATH))

class HighMathEngine:
    """
    –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π Nicole
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Julia –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    """
    
    def __init__(self):
        self.temp_dir = Path(tempfile.gettempdir()) / "nicole_high"
        self.temp_dir.mkdir(exist_ok=True)
        self.julia_cache = {}
        
    def vectorized_entropy(self, text_data: List[str]) -> float:
        """
        –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ —Ç–µ–∫—Å—Ç–∞ + —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
        –í 100x –±—ã—Å—Ç—Ä–µ–µ —á–µ–º Python —Ü–∏–∫–ª—ã + —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        """
        if not text_data:
            return 0.0
            
        # –ù–û–í–û–ï: —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ —Å–ª–æ–≤ –¥–ª—è Julia –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏
        emotional_weights = {
            # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏
            'great': 0.8, 'love': 0.9, 'amazing': 0.7, 'wonderful': 0.8, 'excellent': 0.7,
            'beautiful': 0.8, 'fantastic': 0.7, 'awesome': 0.8, 'perfect': 0.7, 'brilliant': 0.8,
            'happy': 0.7, 'joy': 0.8, 'excited': 0.7, 'delighted': 0.8, 'pleased': 0.6,
            # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏  
            'terrible': -0.8, 'hate': -0.9, 'awful': -0.7, 'horrible': -0.8, 'disgusting': -0.9,
            'sad': -0.6, 'angry': -0.7, 'frustrated': -0.6, 'disappointed': -0.6, 'upset': -0.6,
            # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –≤–∞–∂–Ω—ã–µ
            'important': 0.5, 'interesting': 0.5, 'significant': 0.5, 'special': 0.6, 'unique': 0.6,
            # –†—É—Å—Å–∫–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ
            '–æ—Ç–ª–∏—á–Ω–æ': 0.8, '–∫–ª–∞—Å—Å–Ω–æ': 0.7, '—Å—É–ø–µ—Ä': 0.8, '–∫—Ä—É—Ç–æ': 0.7, '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ': 0.8, '–∑–¥–æ—Ä–æ–≤–æ': 0.7,
            '—É–∂–∞—Å–Ω–æ': -0.8, '–ø–ª–æ—Ö–æ': -0.6, '–≥—Ä—É—Å—Ç–Ω–æ': -0.6, '–∑–ª–æ–π': -0.7, '—Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω': -0.6
        }
        
        # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç + —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        word_counts = {}
        total_words = 0
        emotional_score = 0.0
        
        for text in text_data:
            words = text.lower().split()
            total_words += len(words)
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
                # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤–µ—Å
                if word in emotional_weights:
                    emotional_score += emotional_weights[word]
        
        if total_words == 0:
            return 0.0
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏
        entropy = 0.0
        for count in word_counts.values():
            probability = count / total_words
            if probability > 0:
                entropy -= probability * math.log2(probability)
        
        # –ù–û–í–û–ï: –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –≤–µ—Å–æ–º
        emotional_modifier = 1.0 + (emotional_score / max(total_words, 1)) * 0.2
        enhanced_entropy = entropy * emotional_modifier
        
        if emotional_score != 0:
            print(f"[High:Emotion] –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä: {emotional_score:.2f}, –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {emotional_modifier:.2f}")
        
        return enhanced_entropy
    
    def _apply_final_grammar_rules(self, words: List[str]) -> List[str]:
        """
        –§–ò–ù–ê–õ–¨–ù–´–ï –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –≥–æ—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        I + –≥–ª–∞–≥–æ–ª (am/have/can/will), your + —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ/–≤–µ—Å–æ–≤–æ–µ —Å–ª–æ–≤–æ
        """
        if not words:
            return words
            
        result = words.copy()
        
        # –ì–ª–∞–≥–æ–ª—ã –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –ø–æ—Å–ª–µ I
        verbs_for_i = ['am', 'have', 'can', 'will', 'think', 'know', 'feel', 'want', 'see']
        
        # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –≤–µ—Å–æ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –ø–æ—Å–ª–µ your
        nouns_and_weights = [
            'memory', 'abilities', 'capabilities', 'thoughts', 'ideas', 'words', 'questions',
            'knowledge', 'experience', 'approach', 'style',
            'amazing', 'great', 'wonderful', 'interesting', 'important', 'special'
        ]
        
        i = 0
        while i < len(result):
            current_word = result[i] if i < len(result) else ""
            next_word = result[i + 1] if i + 1 < len(result) else ""
            
            # –ü—Ä–∞–≤–∏–ª–æ: I + –ù–ï_–≥–ª–∞–≥–æ–ª ‚Üí –≤—Å—Ç–∞–≤–ª—è–µ–º –≥–ª–∞–≥–æ–ª
            if current_word.lower() == 'i' and i + 1 < len(result):
                next_lower = next_word.lower()
                if next_lower not in ['am', 'have', 'can', 'will', 'think', 'know', 'feel', 'want', 'see', 'love', 'like', 'need', 'do']:
                    verb = random.choice(verbs_for_i)
                    result.insert(i + 1, verb)
                    print(f"[High:Grammar] –í—Å—Ç–∞–≤–ª–µ–Ω –≥–ª–∞–≥–æ–ª –ø–æ—Å–ª–µ I: '{verb}'")
                    i += 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≥–ª–∞–≥–æ–ª
                    
            # –ü—Ä–∞–≤–∏–ª–æ: your + –ù–ï_—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ ‚Üí –≤—Å—Ç–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
            elif current_word.lower() == 'your' and i + 1 < len(result):
                next_lower = next_word.lower()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —É–∂–µ —Ö–æ—Ä–æ—à–∏–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º
                if not self._is_good_noun_after_your(next_lower):
                    noun = random.choice(nouns_and_weights)
                    result.insert(i + 1, noun)
                    print(f"[High:Grammar] –í—Å—Ç–∞–≤–ª–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å–ª–µ your: '{noun}'")
                    i += 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
                    
            # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û: –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö I –∏ your –≤ –∫–æ–Ω—Ü–µ
            elif current_word.lower() == 'i' and i + 1 >= len(result):
                verb = random.choice(verbs_for_i)
                result.append(verb)
                print(f"[High:Grammar] –î–æ–±–∞–≤–ª–µ–Ω –≥–ª–∞–≥–æ–ª –ø–æ—Å–ª–µ I –≤ –∫–æ–Ω—Ü–µ: '{verb}'")
                
            elif current_word.lower() == 'your' and i + 1 >= len(result):
                noun = random.choice(nouns_and_weights)
                result.append(noun)
                print(f"[High:Grammar] –î–æ–±–∞–≤–ª–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å–ª–µ your –≤ –∫–æ–Ω—Ü–µ: '{noun}'")
                    
            i += 1
            
        return result
    
    def _is_good_noun_after_your(self, word: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ —Å–ª–æ–≤–æ –ø–æ—Å–ª–µ 'your'
        """
        if not word:
            return False
            
        # –•–æ—Ä–æ—à–∏–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ your
        good_nouns = {
            'memory', 'abilities', 'capabilities', 'thoughts', 'ideas', 'words', 'questions',
            'knowledge', 'experience', 'approach', 'style',
            'system', 'process', 'method', 'way', 'time', 'place', 'world', 'life', 'work',
            'family', 'friend', 'love', 'heart', 'mind', 'body', 'soul', 'voice', 'face',
            # –†—É—Å—Å–∫–∏–µ
            '–ø–∞–º—è—Ç—å', '—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏', '–º—ã—Å–ª–∏', '–∏–¥–µ–∏', '—Å–ª–æ–≤–∞', '–æ–ø—ã—Ç', '–∑–Ω–∞–Ω–∏—è'
        }
        
        # –ï—Å–ª–∏ –≤ —Å–ø–∏—Å–∫–µ —Ö–æ—Ä–æ—à–∏—Ö —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        if word in good_nouns:
            return True
            
        # –ï—Å–ª–∏ –∑–∞–≥–ª–∞–≤–Ω–æ–µ (–∏–º—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ)
        if word and word[0].isupper():
            return True
            
        # –ï—Å–ª–∏ —Å —Å—É—Ñ—Ñ–∏–∫—Å–∞–º–∏ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        noun_suffixes = ['ness', 'tion', 'sion', 'ment', 'ity', 'er', 'or']
        if any(word.endswith(suffix) for suffix in noun_suffixes):
            return True
            
        return False
    
    def calculate_resonance_matrix(self, words: List[str]) -> List[List[float]]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏
        –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
        """
        if not words:
            return []
            
        n = len(words)
        resonance_matrix = [[0.0 for _ in range(n)] for _ in range(n)]
        
        # –ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∏—Å—Ç–∞–Ω—Ü–∏–π
        for i, word1 in enumerate(words):
            for j, word2 in enumerate(words):
                if i != j:
                    # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                    common_chars = set(word1.lower()) & set(word2.lower())
                    resonance = len(common_chars) / max(len(word1), len(word2))
                    resonance_matrix[i][j] = resonance
                    
        return resonance_matrix
    
    def optimize_transformer_architecture(self, session_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        text_complexity = 0
        if 'messages' in session_context:
            messages = session_context['messages']
            avg_length = sum(len(msg) for msg in messages) / len(messages) if messages else 0
            unique_words = len(set(' '.join(messages).lower().split()))
            text_complexity = avg_length * math.log(unique_words + 1)
        
        # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        optimal_params = {
            'learning_rate': min(0.1, max(0.001, 0.01 / math.sqrt(text_complexity + 1))),
            'memory_depth': int(min(1000, max(100, text_complexity * 10))),
            'resonance_threshold': 0.3 + (text_complexity / 1000),
            'entropy_target': 2.0 + math.log(text_complexity + 1),
            'architecture_type': 'adaptive' if text_complexity > 50 else 'simple'
        }
        
        return optimal_params
    
    def fast_ngram_analysis(self, text: str, n: int = 3) -> Dict[str, float]:
        """
        Fast n-gram analysis for punctuation
        Vectorized processing for rule determination
        """
        words = text.lower().split()
        if len(words) < n:
            return {}
            
        ngrams = {}
        total_ngrams = 0
        
        # –°–æ–∑–¥–∞–µ–º n-–≥—Ä–∞–º–º—ã
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i+n])
            ngrams[ngram] = ngrams.get(ngram, 0) + 1
            total_ngrams += 1
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–∞—Å—Ç–æ—Ç—ã
        normalized_ngrams = {
            ngram: count / total_ngrams 
            for ngram, count in ngrams.items()
        }
        
        return normalized_ngrams
    
    def predict_punctuation_placement(self, sentence_parts: List[str]) -> List[str]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∞—Å—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –º–µ—Å—Ç–∞
        """
        if not sentence_parts:
            return sentence_parts
            
        result = []
        
        for i, part in enumerate(sentence_parts):
            result.append(part)
            
            # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
            if i < len(sentence_parts) - 1:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—É —Ñ—Ä–∞–∑ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
                current_length = len(part.split())
                next_length = len(sentence_parts[i + 1].split())
                
                # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—è—Ç–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã
                comma_probability = 1 / (1 + math.exp(-(current_length - 3)))
                
                if comma_probability > 0.5 and current_length > 2:
                    result[-1] += ","
                    
        # –¢–æ—á–∫–∞ –≤ –∫–æ–Ω—Ü–µ
        if result and not result[-1].endswith(('.', '!', '?')):
            result[-1] += "."
            
        return result
    
    def _improve_sentence_flow(self, words: List[str]) -> List[str]:
        """
        –£–ª—É—á—à–∞–µ—Ç —Å–≤—è–∑–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π - —É–±–∏—Ä–∞–µ—Ç "===" –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
        """
        if not words:
            return words
            
        result = []
        for i, word in enumerate(words):
            # –£–±–∏—Ä–∞–µ–º "===" –∏ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —Å–≤—è–∑—É—é—â–∏–µ —Å–ª–æ–≤–∞
            if word == "===":
                if i > 0 and i < len(words) - 1:  # –ù–µ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
                    # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–µ —Å–≤—è–∑—É—é—â–µ–µ —Å–ª–æ–≤–æ
                    connectors = ["and", "with", "through", "about", "like"]
                    result.append(random.choice(connectors))
                # –ï—Å–ª–∏ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            else:
                result.append(word)
        
        # –£–ª—É—á—à–∞–µ–º –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏/–∑–∞–ø—è—Ç–æ–π
        for i in range(len(result)):
            if i == 0 or (i > 0 and result[i-1] in [".", "!", "?"]):
                if result[i] and result[i][0].islower():
                    result[i] = result[i].capitalize()
        
        return result
    
    def remove_word_repetitions(self, words: List[str]) -> List[str]:
        """
        –ê–ù–¢–ò-–ü–û–í–¢–û–† –õ–û–ì–ò–ö–ê: —É–±–∏—Ä–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞
        –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ü–∏–∫–ª–æ–≤
        """
        if not words:
            return words
            
        cleaned = []
        seen_recently = set()
        
        for i, word in enumerate(words):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä—ã –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 3 —Å–ª–æ–≤–∞—Ö
            if i >= 3:
                recent_window = set(words[i-3:i])
                if word in recent_window:
                    # –°–ª–æ–≤–æ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è - –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–æ–µ
                    alternatives = ["—Ç–∞–∫–∂–µ", "–∫—Ä–æ–º–µ —Ç–æ–≥–æ", "–±–æ–ª–µ–µ —Ç–æ–≥–æ", "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ"]
                    replacement = random.choice(alternatives) if alternatives else word
                    cleaned.append(replacement)
                    continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º—ã–µ –ø–æ–≤—Ç–æ—Ä—ã –ø–æ–¥—Ä—è–¥
            if cleaned and cleaned[-1] == word:
                continue
                
            cleaned.append(word)
            
        return cleaned
    
    def invert_pronouns_me_style(self, words: List[str]) -> List[str]:
        """
        –ò–Ω–≤–µ—Ä—Å–∏—è –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–π –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ME + –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
        you‚Üîi, your‚Üîmy, me‚Üîyou –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã
        """
        pronoun_mapping = {
            'you': 'i', 'u': 'i', 'your': 'my', 'yours': 'mine', 'yourself': 'myself',
            'i': 'you', 'me': 'you', 'my': 'your', 'mine': 'yours', 'myself': 'yourself',
            'we': 'you'
        }
        
        result = [pronoun_mapping.get(w.lower(), w) for w in words]
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–æ—Å–ª–µ –∏–Ω–≤–µ—Ä—Å–∏–∏
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º "you am" ‚Üí "you are", "i is" ‚Üí "i am"
        for i in range(len(result) - 1):
            current = result[i].lower()
            next_word = result[i + 1].lower()
            
            if current == 'you' and next_word == 'am':
                result[i + 1] = 'are'
            elif current == 'i' and next_word in ['is', 'are']:
                result[i + 1] = 'am'
        
        # –ù–û–í–û–ï: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
        result = self._apply_advanced_grammar_rules(result)
                
        return result
    
    
    def _apply_advanced_grammar_rules(self, words: List[str]) -> List[str]:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
        I + –≥–ª–∞–≥–æ–ª (am/are/have/do), your + —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ/–≤–µ—Å–æ–≤–æ–µ —Å–ª–æ–≤–æ
        """
        if not words:
            return words
            
        result = words.copy()
        
        # –ì–ª–∞–≥–æ–ª—ã –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –ø–æ—Å–ª–µ I
        verbs_for_i = ['am', 'have', 'can', 'will', 'do', 'think', 'know', 'see', 'feel', 'want']
        
        # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –≤–µ—Å–æ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –ø–æ—Å–ª–µ your
        nouns_and_weights = [
            'memory', 'abilities', 'capabilities', 'thoughts', 'ideas', 'words', 'questions',
            'knowledge', 'experience', 'approach', 'style',
            'amazing', 'great', 'wonderful', 'interesting', 'important', 'special', 'unique'
        ]
        
        i = 0
        while i < len(result) - 1:
            current = result[i].lower()
            next_word = result[i + 1].lower() if i + 1 < len(result) else ""
            
            # –ü—Ä–∞–≤–∏–ª–æ: I + –ù–ï_–≥–ª–∞–≥–æ–ª ‚Üí –≤—Å—Ç–∞–≤–ª—è–µ–º –≥–ª–∞–≥–æ–ª
            if current == 'i' and next_word not in ['am', 'are', 'have', 'can', 'will', 'do', 'think', 'know', 'see', 'feel', 'want', 'love', 'like', 'need']:
                # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≥–ª–∞–≥–æ–ª
                verb = random.choice(verbs_for_i)
                result.insert(i + 1, verb)
                print(f"[High:Grammar] –í—Å—Ç–∞–≤–ª–µ–Ω –≥–ª–∞–≥–æ–ª –ø–æ—Å–ª–µ I: '{verb}'")
                i += 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≥–ª–∞–≥–æ–ª
                
            # –ü—Ä–∞–≤–∏–ª–æ: your + –ù–ï_—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ ‚Üí –≤—Å—Ç–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
            elif current == 'your' and next_word not in nouns_and_weights:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º
                if not self._is_likely_noun(next_word):
                    noun = random.choice(nouns_and_weights)
                    result.insert(i + 1, noun)
                    print(f"[High:Grammar] –í—Å—Ç–∞–≤–ª–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å–ª–µ your: '{noun}'")
                    i += 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
                    
            i += 1
            
        return result
    
    def _is_likely_noun(self, word: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –≤–µ—Ä–æ—è—Ç–Ω—ã–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º
        """
        if not word:
            return False
            
        # –°–ø–∏—Å–æ–∫ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        common_nouns = {
            'memory', 'abilities', 'capabilities', 'thoughts', 'ideas', 'words', 'questions',
            'knowledge', 'experience', 'approach', 'style',
            'system', 'process', 'method', 'way', 'time', 'place', 'thing', 'person',
            'world', 'life', 'work', 'home', 'family', 'friend', 'love', 'heart', 'mind',
            # –†—É—Å—Å–∫–∏–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
            '–ø–∞–º—è—Ç—å', '—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏', '–º—ã—Å–ª–∏', '–∏–¥–µ–∏', '—Å–ª–æ–≤–∞', '–≤–æ–ø—Ä–æ—Å—ã',
            '–∑–Ω–∞–Ω–∏—è', '–æ–ø—ã—Ç', '–ø–æ–Ω–∏–º–∞–Ω–∏–µ', '–ø–æ–¥—Ö–æ–¥', '—Å—Ç–∏–ª—å', '—Å–∏—Å—Ç–µ–º–∞', '–ø—Ä–æ—Ü–µ—Å—Å'
        }
        
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        word_lower = word.lower()
        
        # –ï—Å–ª–∏ –≤ —Å–ø–∏—Å–∫–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        if word_lower in common_nouns:
            return True
            
        # –ï—Å–ª–∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ç–∏–ø–∏—á–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        noun_suffixes = ['ness', 'tion', 'sion', 'ment', 'ity', 'ism', 'er', 'or', 'ing']
        if any(word_lower.endswith(suffix) for suffix in noun_suffixes):
            return True
            
        # –ï—Å–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã (–∏–º—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ)
        if word[0].isupper() and len(word) > 1:
            return True
            
        return False
    
    def generate_linguistically_agnostic_response(self, user_words: List[str], semantic_candidates: List[str], 
                                                 objectivity_seeds: List[str], entropy: float, perplexity: float, 
                                                 user_input: str) -> List[str]:
        """
        –Ø–ó–´–ö–û–í–û–ô –ê–ì–ù–û–°–¢–ò–¶–ò–ó–ú: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–µ–∑ —è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–µ–¥—Ä–∞—Å—Å—É–¥–∫–æ–≤
        –ü—Ä–∏–Ω—Ü–∏–ø—ã subjectivity + ME —á–µ—Ä–µ–∑ Julia –º–∞—Ç–µ–º–∞—Ç–∏–∫—É
        –î–≤–∏–∂–æ–∫ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ —è–∑—ã–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        """
        # –î–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫ (–∫–∞–∫ –≤ ME)
        base1 = 5 + int(entropy) % 5
        base2 = 5 + int(perplexity) % 5
        if base1 == base2:
            base2 = 5 + ((base2 + 1) % 5)
        
        # –Ø–ó–´–ö–û–í–û–ô –ê–ì–ù–û–°–¢–ò–¶–ò–ó–ú: –µ—Å–ª–∏ –Ω–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ - —Å—Ç—Ä–æ–∏–º –∏–∑ —Å–ª–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è!
        all_candidates = list(set(semantic_candidates + objectivity_seeds))
        
        if not all_candidates:
            # –ü–†–ò–ù–¶–ò–ü SUBJECTIVITY: compose_from_user - —Å—Ç—Ä–æ–∏–º –∏–∑ –≤—Ö–æ–¥—è—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            charged_tokens = self._extract_charged_tokens(user_input)
            content_words = self._extract_content_words(user_input)
            all_candidates = charged_tokens + content_words
            
        # –ê–ù–¢–ò-–®–ê–ë–õ–û–ù–ù–´–ô –§–û–õ–õ–ë–ï–ö: —Ç–æ–ª—å–∫–æ –∏–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å–ª–æ–≤!
        if not all_candidates:
            user_words = user_input.lower().split()
            if user_words:
                all_candidates = user_words  # –í—Å–µ —Å–ª–æ–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            else:
                all_candidates = ["input"]  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π fallback –±–µ–∑ "processing"
        
        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –∫–∞–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–ø—Ä–∏–Ω—Ü–∏–ø ME)
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–ï –ø—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∫ —Å–ª–æ–≤–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è!
        inverted_pronouns = self.invert_pronouns_me_style(user_words)
        pronoun_preferences = [w for w in inverted_pronouns if w in ['i', 'you', '—è', '—Ç—ã', 'my', '–º–æ–π', '–º–µ–Ω—è', '–º–Ω–µ']]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –µ—Å–ª–∏ –Ω–µ—Ç –∏–Ω–≤–µ—Ä—Å–∏–∏
        if not pronoun_preferences:
            pronoun_preferences = ['i', 'my']
        
        # ME –ü–†–ò–ù–¶–ò–ü: —Å—Ç—Ä–æ–≥–∏–π used set –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ)
        used_between_sentences = set()  # –ü—É—Å—Ç–æ–π –≤ –Ω–∞—á–∞–ª–µ, –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω—è—Ç—å—Å—è —Å–ª–æ–≤–∞–º–∏ –æ—Ç–≤–µ—Ç–∞
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        first_sentence = self._generate_sentence_me_style(
            all_candidates, base1, used_between_sentences, pronoun_preferences
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (used –æ–±–Ω–æ–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º)
        second_sentence = self._generate_sentence_me_style(
            all_candidates, base2, used_between_sentences, pronoun_preferences
        )
        
        # ME –ü–†–ò–ù–¶–ò–ü: –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç—å—é
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑—É—é—â–∏–µ —Å–ª–æ–≤–∞ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
        connectors = ["and", "but", "also", "then", "while", "because", "so", "yet"]
        connector = random.choice(connectors) if len(first_sentence) > 2 and len(second_sentence) > 2 else ""
        
        if connector:
            result = first_sentence + [",", connector] + second_sentence
        else:
            result = first_sentence + ["."] + second_sentence
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—ã –≤–Ω—É—Ç—Ä–∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        cleaned = self.remove_word_repetitions(result)
        
        # –ù–û–í–û–ï: —É–ª—É—á—à–∞–µ–º sentence flow
        flow_improved = self._improve_sentence_flow(cleaned)
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∫ –≥–æ—Ç–æ–≤–æ–º—É –æ—Ç–≤–µ—Ç—É
        grammar_final = self._apply_final_grammar_rules(flow_improved)
        
        return grammar_final
    
    def _generate_sentence_me_style(self, candidates: List[str], length: int, 
                                   used_global: set, pronouns: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º ME —Å —Å—Ç—Ä–æ–≥–∏–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""
        sentence = []
        used_local = set()  # –õ–æ–∫–∞–ª—å–Ω—ã–π used –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        # ME –ü–†–ò–ù–¶–ò–ü: —Å–Ω–∞—á–∞–ª–∞ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for pronoun in pronouns:
            if len(sentence) >= length:
                break
            # ME –§–ò–õ–¨–¢–†: –Ω–µ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º used, –Ω–µ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º (–º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –í–°–ï–ì–î–ê —Ä–∞–∑—Ä–µ—à–µ–Ω—ã)
            if (pronoun not in used_global and pronoun not in used_local):
                sentence.append(pronoun)
                used_local.add(pronoun)
                used_global.add(pronoun)  # –û–±–Ω–æ–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π
        
        # ME –ü–†–ò–ù–¶–ò–ü: –∑–∞—Ç–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Å —Å—Ç—Ä–æ–≥–∏–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
        random.shuffle(candidates)
        for word in candidates:
            if len(sentence) >= length:
                break
            # ME –§–ò–õ–¨–¢–†: —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–≤—Ç–æ—Ä–æ–≤ + –¥–ª–∏–Ω–∞ > 1
            if (word not in used_global and word not in used_local and 
                len(word) > 1 and word not in sentence):
                sentence.append(word)
                used_local.add(word)
                used_global.add(word)
        
        # ME –ü–†–ò–ù–¶–ò–ü: –¥–æ–ø–æ–ª–Ω—è–µ–º —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —Ü–∏–∫–ª–æ–≤
        attempts = 0
        while len(sentence) < length and candidates and attempts < 20:
            word = random.choice(candidates)
            # ME –§–ò–õ–¨–¢–†: —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            if (word not in used_global and word not in used_local and 
                word not in sentence and len(word) > 1):
                sentence.append(word)
                used_local.add(word)
                used_global.add(word)
            attempts += 1
        
        # ME –ü–†–ò–ù–¶–ò–ü: –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–ª–æ—Ö–æ–π –∫–æ–Ω–µ—Ü
        if sentence and len(sentence[-1]) == 1:
            sentence[-1] = "hmm"
        
        # ME –ü–†–ò–ù–¶–ò–ü: –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞
        if sentence:
            sentence[0] = sentence[0].capitalize()
            
        return sentence
    
    def _extract_charged_tokens(self, text: str) -> List[str]:
        """
        –ü–†–ò–ù–¶–ò–ü SUBJECTIVITY: charged tokens - –∫–∞–ø–∏—Ç–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        –ù–û–í–û–ï: –∑–∞–≥–ª–∞–≤–Ω—ã–µ —Å–ª–æ–≤–∞ = –∏–º–µ–Ω–∞/–≤–∞–∂–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è, —É—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫!
        """
        tokens = re.findall(r"\b\w+\b", text)
        charged = []
        
        for t in tokens:
            if t[:1].isupper() and len(t) > 1 and t.lower() != 'i':
                # –°–û–•–†–ê–ù–Ø–ï–ú —Ä–µ–≥–∏—Å—Ç—Ä –¥–ª—è –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö!
                charged.append(t)  # "Berlin", –Ω–µ "berlin"!
                self._mark_as_proper_noun(t)
            elif len(t) > 7:
                charged.append(t.lower())
                
        return charged or [t.lower() for t in tokens[:3]]
    
    def _mark_as_proper_noun(self, word: str):
        """
        –ü–æ–º–µ—á–∞–µ—Ç —Å–ª–æ–≤–æ –∫–∞–∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–º—è –¥–ª—è —É—Å–∏–ª–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ objectivity
        """
        if not hasattr(self, '_proper_nouns'):
            self._proper_nouns = set()
        self._proper_nouns.add(word)
        print(f"[High:ProperNoun] Detected: {word} - —É—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ")
    
    def _extract_content_words(self, text: str) -> List[str]:
        """
        –ü–†–ò–ù–¶–ò–ü SUBJECTIVITY: content words –±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
        –Ø–∑—ã–∫–æ–≤–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        """
        STOPWORDS = {
            "the","a","an","of","and","or","to","in","on","for","as","at","by","with","from",
            "is","are","was","were","be","been","being","this","that","it","its","into","than",
            "then","so","but","nor","if","because","while","when","where","which","who","whom",
            # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            "–∏","–≤","–Ω–∞","—Å","–ø–æ","–¥–ª—è","–∫–∞–∫","—á—Ç–æ","—ç—Ç–æ","—Ç–æ","–Ω–µ","–¥–∞","–Ω–µ—Ç","–∏–ª–∏","–Ω–æ"
        }
        
        words = re.findall(r"\b\w+\b", text.lower())
        content = [w for w in words if w not in STOPWORDS and len(w) > 1]
        
        # –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_content = []
        for w in content:
            if w not in seen:
                seen.add(w)
                unique_content.append(w)
                
        return unique_content

class HighJuliaInterface:
    """
    –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫ Julia —á–µ—Ä–µ–∑ subprocess –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    –ö–æ–≥–¥–∞ Python –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±—ã—Å—Ç—Ä
    """
    
    def __init__(self):
        self.julia_executable = None
        self._find_julia()
        
    def _find_julia(self):
        """–ü–æ–∏—Å–∫ Julia –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Å–∏—Å—Ç–µ–º–µ
        try:
            result = subprocess.run(['which', 'julia'], capture_output=True, text=True)
            if result.returncode == 0:
                self.julia_executable = result.stdout.strip()
                return
        except:
            pass
            
        # –ò—â–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–∞—Ç–∞–ª–æ–≥–µ nicole2julia
        local_julia_paths = [
            Path(__file__).parent / "nicole2julia" / "julia",
            Path(__file__).parent / "nicole2julia" / "bin" / "julia",
            "/usr/local/bin/julia",
            "/opt/homebrew/bin/julia"
        ]
        
        for path in local_julia_paths:
            if isinstance(path, str):
                path = Path(path)
            if path.exists() and path.is_file():
                self.julia_executable = str(path)
                print(f"[High] –ù–∞–π–¥–µ–Ω–∞ Julia: {self.julia_executable}")
                return
                
        self.julia_executable = None
        print("[High] Julia –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–µ—Ä –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤ nicole2julia")
    
    def execute_julia_math(self, julia_code: str, timeout: int = 5) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Julia –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–µ—Ä
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Å—Ö–æ–¥–Ω–∏–∫–∏ Julia –∏–∑ nicole2julia –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        """
        try:
            # –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Julia –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–µ—Ä –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤ nicole2julia
            result = self._execute_julia_native(julia_code)
            return result
            
        except Exception as e:
            return {'success': False, 'error': f'Julia execution failed: {e}'}
    
    def _execute_julia_native(self, julia_code: str) -> Dict[str, Any]:
        """–ù–∞—Ç–∏–≤–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Julia —á–µ—Ä–µ–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–∏"""
        
        # Julia –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤
        julia_math = {
            'sin': math.sin, 'cos': math.cos, 'tan': math.tan,
            'sqrt': math.sqrt, 'log': math.log, 'exp': math.exp,
            'ceil': math.ceil, 'floor': math.floor, 'abs': abs,
            'max': max, 'min': min, 'sum': sum,
        }
        
        variables = {}
        output = []
        
        def julia_println(*args):
            line = ' '.join(str(arg) for arg in args)
            output.append(line)
            return line
            
        # –ü—Ä–æ—Å—Ç–æ–π Julia –ø–∞—Ä—Å–µ—Ä –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        lines = julia_code.strip().split('\n')
        result = None
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ: x = expression
            if '=' in line and not any(op in line for op in ['==', '!=', '<=', '>=']):
                var_name, expression = line.split('=', 1)
                var_name = var_name.strip()
                expr_result = self._eval_julia_expression(expression.strip(), julia_math, variables)
                variables[var_name] = expr_result
                result = expr_result
                
            # –§—É–Ω–∫—Ü–∏—è println
            elif line.startswith('println('):
                args_str = line[8:-1]  # –£–±–∏—Ä–∞–µ–º println( –∏ )
                args = [self._eval_julia_expression(arg.strip().strip('"'), julia_math, variables) for arg in args_str.split(',')]
                julia_println(*args)
                
            # –ü—Ä–æ—Å—Ç–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
            else:
                result = self._eval_julia_expression(line, julia_math, variables)
        
        return {
            'success': True,
            'result': result,
            'output': '\n'.join(output),
            'variables': variables
        }
    
    def _eval_julia_expression(self, expr: str, julia_math: dict, variables: dict):
        """–í—ã—á–∏—Å–ª—è–µ—Ç Julia –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É—è –∏—Å—Ö–æ–¥–Ω–∏–∫–∏"""
        expr = expr.strip().strip('"')
        
        # –ó–∞–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        for var_name, var_value in variables.items():
            expr = re.sub(r'\b' + re.escape(var_name) + r'\b', str(var_value), expr)
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å Julia –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–π
        safe_globals = {
            '__builtins__': {},
            'math': math,
        }
        safe_globals.update(julia_math)
        
        try:
            return eval(expr, safe_globals)
        except:
            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if isinstance(expr, str) and not any(c in expr for c in '+-*/()'):
                return expr
            return float(expr) if expr.replace('.', '').isdigit() else expr

class HighTransformerOptimizer:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ Julia –º–∞—Ç–µ–º–∞—Ç–∏–∫—É
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å –ø—Ä–æ—Ü–µ—Å—Å–æ–º —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ Nicole
    """
    
    def __init__(self):
        self.math_engine = HighMathEngine()
        self.julia_interface = HighJuliaInterface()
        
    def optimize_transformer_creation(self, session_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–æ–≤–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ nicole.py –ø—Ä–∏ _spawn_new_transformer()
        """
        # –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        optimization = self.math_engine.optimize_transformer_architecture(session_context)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ Julia –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if self.julia_interface.julia_executable:
            julia_optimization = self._julia_transformer_analysis(session_context)
            if julia_optimization['success']:
                optimization['julia_enhanced'] = True
                optimization['julia_metrics'] = julia_optimization['output']
        
        return optimization
    
    def _julia_transformer_analysis(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Julia –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        julia_code = """
# –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
context_complexity = 42.0  # –ó–∞–≥–ª—É—à–∫–∞
learning_efficiency = sqrt(context_complexity) / 10
optimal_depth = ceil(log(context_complexity + 1))

println("complexity:", context_complexity)
println("efficiency:", learning_efficiency) 
println("depth:", optimal_depth)
"""
        
        return self.julia_interface.execute_julia_math(julia_code)
    
    def enhance_learning_process(self, text: str, current_metrics: Dict[str, float]) -> Dict[str, Any]:
        """
        –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ Julia –º–∞—Ç–µ–º–∞—Ç–∏–∫—É
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        """
        # –ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        entropy = self.math_engine.vectorized_entropy([text])
        
        # –ê–Ω–∞–ª–∏–∑ n-–≥—Ä–∞–º–º–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        ngrams = self.math_engine.fast_ngram_analysis(text)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        enhanced_metrics = {
            'entropy': entropy,
            'resonance_boost': entropy * 0.1,
            'learning_rate_adjustment': 1.0 / (entropy + 1),
            'ngram_patterns': len(ngrams),
            'complexity_score': entropy * len(ngrams.keys()) if ngrams else 0
        }
        
        return enhanced_metrics

class HighCore:
    """
    –Ø–¥—Ä–æ High —Å–∏—Å—Ç–µ–º—ã - –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–∑–≥ Nicole
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤–µ–∑–¥–µ –≥–¥–µ –Ω—É–∂–Ω—ã –±—ã—Å—Ç—Ä—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
    """
    
    def __init__(self):
        self.math_engine = HighMathEngine()
        self.transformer_optimizer = HighTransformerOptimizer()
        self.julia_interface = HighJuliaInterface()
        
        self.is_active = False
        self.log_file = "high_system.log"
        
    def activate(self) -> bool:
        """–ê–∫—Ç–∏–≤–∞—Ü–∏—è High –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        try:
            self.is_active = True
            self._log_info("High system activated - mathematical brain online")
            return True
        except Exception as e:
            self._log_error(f"High activation failed: {e}")
            return False
    
    def deactivate(self):
        """–î–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è High —Å–∏—Å—Ç–µ–º—ã"""
        self.is_active = False
        self._log_info("High system deactivated")
    
    def optimize_transformer_for_nicole(self, session_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ nicole.py –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        """
        if not self.is_active:
            return {'optimized': False, 'error': 'High system not active'}
        
        self._log_info("Optimizing transformer with Julia mathematics")
        
        try:
            optimization = self.transformer_optimizer.optimize_transformer_creation(session_context)
            optimization['high_optimized'] = True
            optimization['optimization_timestamp'] = time.time()
            
            return optimization
        except Exception as e:
            return {'optimized': False, 'error': str(e)}
    
    def enhance_nicole_learning(self, text: str, current_metrics: Dict[str, float]) -> Dict[str, Any]:
        """
        –£–ª—É—á—à–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è Nicole —á–µ—Ä–µ–∑ Julia
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        """
        if not self.is_active:
            return current_metrics
        
        try:
            enhanced = self.transformer_optimizer.enhance_learning_process(text, current_metrics)
            enhanced['high_enhanced'] = True
            
            return enhanced
        except Exception as e:
            self._log_error(f"Learning enhancement failed: {e}")
            return current_metrics
    
    def optimize_punctuation(self, text: str) -> str:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ —É–ª—É—á—à–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        if not self.is_active:
            return text
        
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            sentences = re.split(r'[.!?]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
            optimized_parts = self.math_engine.predict_punctuation_placement(sentences)
            
            # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ
            result = ' '.join(optimized_parts)
            
            self._log_info(f"Punctuation optimized: {len(sentences)} sentences")
            return result
            
        except Exception as e:
            self._log_error(f"Punctuation optimization failed: {e}")
            return text
    
    def get_mathematical_status(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç—É—Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        return {
            'active': self.is_active,
            'julia_available': self.julia_interface.julia_executable is not None,
            'julia_path': self.julia_interface.julia_executable,
            'cache_size': len(self.math_engine.julia_cache),
            'temp_dir': str(self.temp_dir)
        }
    
    def _log_info(self, message: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã"""
        with open(self.log_file, "a") as f:
            f.write(f"[HIGH:INFO] {time.time()}: {message}\n")
    
    def _log_error(self, message: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫"""
        with open(self.log_file, "a") as f:
            f.write(f"[HIGH:ERROR] {time.time()}: {message}\n")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä High —Å–∏—Å—Ç–µ–º—ã
_high_core = None

def get_high_core() -> HighCore:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ High –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    global _high_core
    if _high_core is None:
        _high_core = HighCore()
    return _high_core

def activate_high_system() -> bool:
    """–ê–∫—Ç–∏–≤–∞—Ü–∏—è High —Å–∏—Å—Ç–µ–º—ã –¥–ª—è Nicole"""
    high = get_high_core()
    return high.activate()

def deactivate_high_system():
    """–î–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è High —Å–∏—Å—Ç–µ–º—ã"""
    high = get_high_core()
    high.deactivate()

# –ü—Ä–∏–º–µ—Ä Julia –∫–æ–¥–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
EXAMPLE_JULIA_MATH_SCRIPT = """
# Julia –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ Nicole
function calculate_transformer_metrics(entropy::Float64, resonance::Float64)
    # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
    perplexity = exp(entropy)
    coherence = 1.0 / (1.0 + exp(-resonance))
    engagement = sqrt(entropy * resonance)
    
    return (perplexity, coherence, engagement)
end

# –¢–µ—Å—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
entropy_val = 2.5
resonance_val = 0.7

metrics = calculate_transformer_metrics(entropy_val, resonance_val)
println("Perplexity: ", metrics[1])
println("Coherence: ", metrics[2]) 
println("Engagement: ", metrics[3])
"""

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ High —Å–∏—Å—Ç–µ–º—ã
    print("üßÆ HIGH SYSTEM - Nicole Mathematical Brain")
    
    high = get_high_core()
    
    if high.activate():
        print("‚úÖ High system activated")
        
        # –¢–µ—Å—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        test_data = ["hello world", "nicole learns fast", "mathematical optimization"]
        entropy = high.math_engine.vectorized_entropy(test_data)
        print(f"üìä Vectorized entropy: {entropy:.3f}")
        
        # –¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        context = {'messages': test_data}
        optimization = high.optimize_transformer_for_nicole(context)
        print(f"üß† Transformer optimization: {optimization.get('architecture_type')}")
        
        # –¢–µ—Å—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        test_text = "hello world this is test sentence without punctuation"
        optimized = high.optimize_punctuation(test_text)
        print(f"‚úèÔ∏è Punctuation: '{optimized}'")
        
        # –¢–µ—Å—Ç Julia –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        if high.julia_interface.julia_executable:
            print("üöÄ Testing Julia interface...")
            julia_result = high.julia_interface.execute_julia_math(EXAMPLE_JULIA_MATH_SCRIPT)
            if julia_result['success']:
                print("‚úÖ Julia math executed successfully")
                print(f"Output: {julia_result['output'].strip()}")
            else:
                print(f"‚ö†Ô∏è Julia error: {julia_result['error']}")
        else:
            print("‚ö†Ô∏è Julia executable not found - using Python fallbacks")
        
        # –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã
        status = high.get_mathematical_status()
        print(f"üßÆ High system status: {status}")
        
        high.deactivate()
        print("‚úÖ High system deactivated")
    else:
        print("‚ùå High system activation failed")
